{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFc1-CI9z3GR",
        "outputId": "0d597b57-1b92-4974-cac1-3b361685de7f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wXLlR2zz8go",
        "outputId": "0f6c1459-a51a-4edb-c404-fa29c50d66c6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n",
            "E_9.ipynb  Илиада.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavnuByVymwK",
        "outputId": "47e4adca-5a22-4cb0-dc62-ab28785e214f"
      },
      "source": [
        "text = open('Илиада.txt', 'rb').read().decode(encoding='ср-1251')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 856904 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "310c57d5-9fc4-4bbb-cb0b-fb7ffab34404"
      },
      "source": [
        "print(text[:379])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пой, богиня, про гнев Ахиллеса, Пелеева сына,\r\n",
            "\r\n",
            "Гнев проклятый, страданий без счета принесший ахейцам,\r\n",
            "\r\n",
            "Много сильных душ героев пославший к Аиду,\r\n",
            "\r\n",
            "Их же самих на съеденье отдавший добычею жадным\r\n",
            "\r\n",
            "5 Птицам окрестным и псам. Это делалось, волею Зевса,\r\n",
            "\r\n",
            "С самых тех пор, как впервые, поссорясь, расстались враждебно\r\n",
            "\r\n",
            "Сын Атрея, владыка мужей, и Пелид многосветлый.\r\n",
            "\r\n",
            "\r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "9b0122aa-3a38-487a-dfd7-d1794b1851b9"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_as_int, text[:2000], len(text_as_int), len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsninlZpefkI",
        "outputId": "437a39ca-0b93-4250-a399-d6ee40842136"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([52, 82, 77, ...,  0,  1,  0]),\n",
              " 'Пой, богиня, про гнев Ахиллеса, Пелеева сына,\\r\\n\\r\\nГнев проклятый, страданий без счета принесший ахейцам,\\r\\n\\r\\nМного сильных душ героев пославший к Аиду,\\r\\n\\r\\nИх же самих на съеденье отдавший добычею жадным\\r\\n\\r\\n5 Птицам окрестным и псам. Это делалось, волею Зевса,\\r\\n\\r\\nС самых тех пор, как впервые, поссорясь, расстались враждебно\\r\\n\\r\\nСын Атрея, владыка мужей, и Пелид многосветлый.\\r\\n\\r\\n\\r\\nКто ж из бессмертных богов возбудил эту ссору меж ними?\\r\\n\\r\\nСын Лето и Зевса. Царем раздраженный, наслал он\\r\\n\\r\\n10 Злую болезнь на ахейскую рать. Погибали народы\\r\\n\\r\\nИз-за того, что Хриса-жреца Атрид обесчестил.\\r\\n\\r\\nТот к кораблям быстролетным ахейцев пришел, чтоб из плена\\r\\n\\r\\nВызволить дочь, за нее заплативши бесчисленный выкуп.\\r\\n\\r\\nШел, на жезле золотом повязку неся Аполлона,\\r\\n\\r\\n15 И обратился с горячей мольбою к собранью ахейцев,\\r\\n\\r\\nБольше всего же - к обоим Атридам, строителям ратей:\\r\\n\\r\\n\"Дети Атрея и пышнопоножные мужи ахейцы!\\r\\n\\r\\nДай вам бессмертные боги, живущие в домах Олимпа,\\r\\n\\r\\nГород приамов разрушить и всем воротиться в отчизну!\\r\\n\\r\\n20 Вы же мне милую дочь отпустите и выкуп примите,\\r\\n\\r\\nЗевсова сына почтивши, далеко разящего Феба\".\\r\\n\\r\\n\\r\\nВсе изъявили ахейцы согласие криком всеобщим\\r\\n\\r\\nЧесть жрецу оказать и принять блистательный выкуп.\\r\\n\\r\\nЛишь Агамемнону было не по сердцу это решенье;\\r\\n\\r\\n25 Нехорошо жреца он прогнал, приказавши сурово:\\r\\n\\r\\n\"Чтобы тебя никогда я, старик, не видал пред судами!\\r\\n\\r\\nНечего здесь тебе медлить, не смей и вперед появляться!\\r\\n\\r\\nИли тебе не помогут ни жезл твой, ни божья повязка.\\r\\n\\r\\nНе отпущу я ее! Состарится дочь твоя в рабстве,\\r\\n\\r\\n30 В Аргосе, в нашем дому, от тебя, от отчизны далеко,\\r\\n\\r\\nТкацкий станок обходя и постель разделяя со мною.\\r\\n\\r\\nПрочь уходи и меня не гневи, чтобы целым вернуться!\"\\r\\n\\r\\n\\r\\nТак он сказал. Испугался старик и, послушный приказу,\\r\\n\\r\\nМолча побрел по песку вдоль громко шумящего моря.\\r\\n\\r\\n35 От кораблей удалясь, опечаленный старец взмолился\\r\\n\\r\\nК сыну прекрасноволосой Лето, Аполлону владыке:\\r\\n\\r\\n\"Слух преклони, сребролукий, о ты, что стоишь на защите\\r\\n\\r\\nХри',\n",
              " 856904,\n",
              " 856904)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UHJDA39zf-O",
        "outputId": "d55ce456-aa65-41e5-c206-26e8f9e641dd"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "П\n",
            "о\n",
            "й\n",
            ",\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4hkDU3i7ozi",
        "outputId": "a9246e9a-587b-468e-cb48-3477acf5226e"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Пой, богиня, про гнев Ахиллеса, Пелеева сына,\\r\\n\\r\\nГнев проклятый, страданий без счета принесший ахейца'\n",
            "'м,\\r\\n\\r\\nМного сильных душ героев пославший к Аиду,\\r\\n\\r\\nИх же самих на съеденье отдавший добычею жадным\\r\\n'\n",
            "'\\r\\n5 Птицам окрестным и псам. Это делалось, волею Зевса,\\r\\n\\r\\nС самых тех пор, как впервые, поссорясь, р'\n",
            "'асстались враждебно\\r\\n\\r\\nСын Атрея, владыка мужей, и Пелид многосветлый.\\r\\n\\r\\n\\r\\nКто ж из бессмертных бого'\n",
            "'в возбудил эту ссору меж ними?\\r\\n\\r\\nСын Лето и Зевса. Царем раздраженный, наслал он\\r\\n\\r\\n10 Злую болезнь '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbw-iR0ymwj",
        "outputId": "8aca242b-f4a5-413d-d5a1-045b016c3d07"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'Пой, богиня, про гнев Ахиллеса, Пелеева сына,\\r\\n\\r\\nГнев проклятый, страданий без счета принесший ахейц'\n",
            "Target data: 'ой, богиня, про гнев Ахиллеса, Пелеева сына,\\r\\n\\r\\nГнев проклятый, страданий без счета принесший ахейца'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Модель на основе GRU**"
      ],
      "metadata": {
        "id": "ZH8Mw8GKX2c1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128\n",
        "rnn_units = 1024"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNfDlxWZ8NXK"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "         tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "**Обучение**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HrXTACTdzY-",
        "outputId": "926adc86-7933-44a3-bd33-8682728d2ed6"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape,)\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 101)\n",
            "scalar_loss:       4.614815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS = 200"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK-hmKjYVoll",
        "outputId": "2f5d9eb0-561b-4308-bfa0-9b0387d7ce50"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "132/132 [==============================] - 34s 195ms/step - loss: 3.0028\n",
            "Epoch 2/200\n",
            "132/132 [==============================] - 27s 192ms/step - loss: 2.1960\n",
            "Epoch 3/200\n",
            "132/132 [==============================] - 27s 196ms/step - loss: 1.7509\n",
            "Epoch 4/200\n",
            "132/132 [==============================] - 28s 200ms/step - loss: 1.5107\n",
            "Epoch 5/200\n",
            "132/132 [==============================] - 29s 208ms/step - loss: 1.3755\n",
            "Epoch 6/200\n",
            "132/132 [==============================] - 29s 211ms/step - loss: 1.2747\n",
            "Epoch 7/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1869\n",
            "Epoch 8/200\n",
            "132/132 [==============================] - 29s 210ms/step - loss: 1.1056\n",
            "Epoch 9/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 1.0257\n",
            "Epoch 10/200\n",
            "132/132 [==============================] - 30s 204ms/step - loss: 0.9433\n",
            "Epoch 11/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.8574\n",
            "Epoch 12/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.7718\n",
            "Epoch 13/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.6821\n",
            "Epoch 14/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.5975\n",
            "Epoch 15/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.5188\n",
            "Epoch 16/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.4433\n",
            "Epoch 17/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.3808\n",
            "Epoch 18/200\n",
            "132/132 [==============================] - 30s 206ms/step - loss: 0.3214\n",
            "Epoch 19/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.2763\n",
            "Epoch 20/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.2413\n",
            "Epoch 21/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.2155\n",
            "Epoch 22/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.1968\n",
            "Epoch 23/200\n",
            "132/132 [==============================] - 29s 209ms/step - loss: 0.1863\n",
            "Epoch 24/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.1813\n",
            "Epoch 25/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 0.1849\n",
            "Epoch 26/200\n",
            "132/132 [==============================] - 30s 208ms/step - loss: 0.2007\n",
            "Epoch 27/200\n",
            "132/132 [==============================] - 30s 208ms/step - loss: 0.2300\n",
            "Epoch 28/200\n",
            "132/132 [==============================] - 30s 207ms/step - loss: 0.2589\n",
            "Epoch 29/200\n",
            "132/132 [==============================] - 29s 209ms/step - loss: 0.2617\n",
            "Epoch 30/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.2406\n",
            "Epoch 31/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.2088\n",
            "Epoch 32/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.1833\n",
            "Epoch 33/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.1629\n",
            "Epoch 34/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.1488\n",
            "Epoch 35/200\n",
            "132/132 [==============================] - 31s 211ms/step - loss: 0.1400\n",
            "Epoch 36/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 0.1360\n",
            "Epoch 37/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.1363\n",
            "Epoch 38/200\n",
            "132/132 [==============================] - 29s 209ms/step - loss: 0.1497\n",
            "Epoch 39/200\n",
            "132/132 [==============================] - 29s 210ms/step - loss: 0.1922\n",
            "Epoch 40/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 0.3324\n",
            "Epoch 41/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.4769\n",
            "Epoch 42/200\n",
            "132/132 [==============================] - 30s 210ms/step - loss: 0.4354\n",
            "Epoch 43/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.3339\n",
            "Epoch 44/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 0.2550\n",
            "Epoch 45/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.2003\n",
            "Epoch 46/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.1625\n",
            "Epoch 47/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.1369\n",
            "Epoch 48/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.1215\n",
            "Epoch 49/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.1101\n",
            "Epoch 50/200\n",
            "132/132 [==============================] - 30s 208ms/step - loss: 0.1029\n",
            "Epoch 51/200\n",
            "132/132 [==============================] - 30s 211ms/step - loss: 0.0990\n",
            "Epoch 52/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 0.0961\n",
            "Epoch 53/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 0.0948\n",
            "Epoch 54/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.0943\n",
            "Epoch 55/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.0962\n",
            "Epoch 56/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.1055\n",
            "Epoch 57/200\n",
            "132/132 [==============================] - 30s 212ms/step - loss: 0.1843\n",
            "Epoch 58/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.9355\n",
            "Epoch 59/200\n",
            "132/132 [==============================] - 30s 211ms/step - loss: 1.0445\n",
            "Epoch 60/200\n",
            "132/132 [==============================] - 30s 206ms/step - loss: 0.7821\n",
            "Epoch 61/200\n",
            "132/132 [==============================] - 29s 208ms/step - loss: 0.6083\n",
            "Epoch 62/200\n",
            "132/132 [==============================] - 29s 208ms/step - loss: 0.4924\n",
            "Epoch 63/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.4089\n",
            "Epoch 64/200\n",
            "132/132 [==============================] - 29s 208ms/step - loss: 0.3436\n",
            "Epoch 65/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.2967\n",
            "Epoch 66/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 0.2641\n",
            "Epoch 67/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.2408\n",
            "Epoch 68/200\n",
            "132/132 [==============================] - 30s 206ms/step - loss: 0.2233\n",
            "Epoch 69/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.2148\n",
            "Epoch 70/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.2091\n",
            "Epoch 71/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.2140\n",
            "Epoch 72/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.2262\n",
            "Epoch 73/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.2538\n",
            "Epoch 74/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.3143\n",
            "Epoch 75/200\n",
            "132/132 [==============================] - 29s 211ms/step - loss: 0.3929\n",
            "Epoch 76/200\n",
            "132/132 [==============================] - 30s 204ms/step - loss: 0.4658\n",
            "Epoch 77/200\n",
            "132/132 [==============================] - 30s 211ms/step - loss: 0.4850\n",
            "Epoch 78/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 0.4945\n",
            "Epoch 79/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 0.4876\n",
            "Epoch 80/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.4774\n",
            "Epoch 81/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.4531\n",
            "Epoch 82/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.4373\n",
            "Epoch 83/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 0.4189\n",
            "Epoch 84/200\n",
            "132/132 [==============================] - 30s 205ms/step - loss: 0.4060\n",
            "Epoch 85/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.3997\n",
            "Epoch 86/200\n",
            "132/132 [==============================] - 28s 205ms/step - loss: 0.4001\n",
            "Epoch 87/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 0.4130\n",
            "Epoch 88/200\n",
            "132/132 [==============================] - 29s 211ms/step - loss: 0.4201\n",
            "Epoch 89/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 0.4417\n",
            "Epoch 90/200\n",
            "132/132 [==============================] - 29s 211ms/step - loss: 0.4620\n",
            "Epoch 91/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 0.4751\n",
            "Epoch 92/200\n",
            "132/132 [==============================] - 30s 206ms/step - loss: 0.4967\n",
            "Epoch 93/200\n",
            "132/132 [==============================] - 30s 209ms/step - loss: 0.5312\n",
            "Epoch 94/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 0.5919\n",
            "Epoch 95/200\n",
            "132/132 [==============================] - 29s 208ms/step - loss: 0.6133\n",
            "Epoch 96/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 0.6328\n",
            "Epoch 97/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.6340\n",
            "Epoch 98/200\n",
            "132/132 [==============================] - 29s 211ms/step - loss: 0.6479\n",
            "Epoch 99/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 0.6790\n",
            "Epoch 100/200\n",
            "132/132 [==============================] - 29s 203ms/step - loss: 0.7237\n",
            "Epoch 101/200\n",
            "132/132 [==============================] - 30s 207ms/step - loss: 0.7418\n",
            "Epoch 102/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.8022\n",
            "Epoch 103/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 0.8667\n",
            "Epoch 104/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 0.9332\n",
            "Epoch 105/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 1.1492\n",
            "Epoch 106/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 1.2572\n",
            "Epoch 107/200\n",
            "132/132 [==============================] - 29s 210ms/step - loss: 1.2007\n",
            "Epoch 108/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1546\n",
            "Epoch 109/200\n",
            "132/132 [==============================] - 30s 208ms/step - loss: 1.1042\n",
            "Epoch 110/200\n",
            "132/132 [==============================] - 30s 206ms/step - loss: 1.0847\n",
            "Epoch 111/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.0474\n",
            "Epoch 112/200\n",
            "132/132 [==============================] - 29s 209ms/step - loss: 1.0752\n",
            "Epoch 113/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1273\n",
            "Epoch 114/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1073\n",
            "Epoch 115/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.0976\n",
            "Epoch 116/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 1.0882\n",
            "Epoch 117/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1028\n",
            "Epoch 118/200\n",
            "132/132 [==============================] - 29s 203ms/step - loss: 1.1643\n",
            "Epoch 119/200\n",
            "132/132 [==============================] - 30s 208ms/step - loss: 1.1405\n",
            "Epoch 120/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.1181\n",
            "Epoch 121/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1238\n",
            "Epoch 122/200\n",
            "132/132 [==============================] - 29s 210ms/step - loss: 1.1522\n",
            "Epoch 123/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1342\n",
            "Epoch 124/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1029\n",
            "Epoch 125/200\n",
            "132/132 [==============================] - 30s 210ms/step - loss: 1.0963\n",
            "Epoch 126/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 1.0841\n",
            "Epoch 127/200\n",
            "132/132 [==============================] - 30s 209ms/step - loss: 1.1004\n",
            "Epoch 128/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.0886\n",
            "Epoch 129/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 1.0983\n",
            "Epoch 130/200\n",
            "132/132 [==============================] - 29s 210ms/step - loss: 1.1240\n",
            "Epoch 131/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 1.1532\n",
            "Epoch 132/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.2072\n",
            "Epoch 133/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 1.1887\n",
            "Epoch 134/200\n",
            "132/132 [==============================] - 30s 209ms/step - loss: 1.2058\n",
            "Epoch 135/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 1.1850\n",
            "Epoch 136/200\n",
            "132/132 [==============================] - 30s 209ms/step - loss: 1.1852\n",
            "Epoch 137/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 1.2171\n",
            "Epoch 138/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.2027\n",
            "Epoch 139/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.2037\n",
            "Epoch 140/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.1937\n",
            "Epoch 141/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.1833\n",
            "Epoch 142/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1961\n",
            "Epoch 143/200\n",
            "132/132 [==============================] - 29s 202ms/step - loss: 1.1986\n",
            "Epoch 144/200\n",
            "132/132 [==============================] - 30s 206ms/step - loss: 1.2098\n",
            "Epoch 145/200\n",
            "132/132 [==============================] - 29s 209ms/step - loss: 1.1902\n",
            "Epoch 146/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1981\n",
            "Epoch 147/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.2955\n",
            "Epoch 148/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.2817\n",
            "Epoch 149/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.2485\n",
            "Epoch 150/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.2223\n",
            "Epoch 151/200\n",
            "132/132 [==============================] - 29s 209ms/step - loss: 1.1961\n",
            "Epoch 152/200\n",
            "132/132 [==============================] - 29s 203ms/step - loss: 1.1852\n",
            "Epoch 153/200\n",
            "132/132 [==============================] - 29s 203ms/step - loss: 1.2068\n",
            "Epoch 154/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.2235\n",
            "Epoch 155/200\n",
            "132/132 [==============================] - 29s 209ms/step - loss: 1.2119\n",
            "Epoch 156/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.2250\n",
            "Epoch 157/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 1.2146\n",
            "Epoch 158/200\n",
            "132/132 [==============================] - 29s 210ms/step - loss: 1.1913\n",
            "Epoch 159/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1818\n",
            "Epoch 160/200\n",
            "132/132 [==============================] - 29s 202ms/step - loss: 1.1869\n",
            "Epoch 161/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 1.1873\n",
            "Epoch 162/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1965\n",
            "Epoch 163/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 1.2021\n",
            "Epoch 164/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.2029\n",
            "Epoch 165/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.1981\n",
            "Epoch 166/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.1870\n",
            "Epoch 167/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.1815\n",
            "Epoch 168/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.1813\n",
            "Epoch 169/200\n",
            "132/132 [==============================] - 29s 203ms/step - loss: 1.1724\n",
            "Epoch 170/200\n",
            "132/132 [==============================] - 30s 204ms/step - loss: 1.1600\n",
            "Epoch 171/200\n",
            "132/132 [==============================] - 29s 208ms/step - loss: 1.1506\n",
            "Epoch 172/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1452\n",
            "Epoch 173/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 1.1506\n",
            "Epoch 174/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 1.1590\n",
            "Epoch 175/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.1612\n",
            "Epoch 176/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1748\n",
            "Epoch 177/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1705\n",
            "Epoch 178/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1735\n",
            "Epoch 179/200\n",
            "132/132 [==============================] - 29s 202ms/step - loss: 1.1844\n",
            "Epoch 180/200\n",
            "132/132 [==============================] - 30s 205ms/step - loss: 1.1966\n",
            "Epoch 181/200\n",
            "132/132 [==============================] - 29s 207ms/step - loss: 1.1859\n",
            "Epoch 182/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.1836\n",
            "Epoch 183/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1841\n",
            "Epoch 184/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.1963\n",
            "Epoch 185/200\n",
            "132/132 [==============================] - 29s 204ms/step - loss: 1.1943\n",
            "Epoch 186/200\n",
            "132/132 [==============================] - 29s 208ms/step - loss: 1.2033\n",
            "Epoch 187/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.2148\n",
            "Epoch 188/200\n",
            "132/132 [==============================] - 29s 203ms/step - loss: 1.2210\n",
            "Epoch 189/200\n",
            "132/132 [==============================] - 30s 208ms/step - loss: 1.2855\n",
            "Epoch 190/200\n",
            "132/132 [==============================] - 29s 202ms/step - loss: 1.3113\n",
            "Epoch 191/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.2997\n",
            "Epoch 192/200\n",
            "132/132 [==============================] - 28s 204ms/step - loss: 1.3183\n",
            "Epoch 193/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.3210\n",
            "Epoch 194/200\n",
            "132/132 [==============================] - 28s 203ms/step - loss: 1.2929\n",
            "Epoch 195/200\n",
            "132/132 [==============================] - 29s 205ms/step - loss: 1.2833\n",
            "Epoch 196/200\n",
            "132/132 [==============================] - 29s 206ms/step - loss: 1.2816\n",
            "Epoch 197/200\n",
            "132/132 [==============================] - 28s 202ms/step - loss: 1.2696\n",
            "Epoch 198/200\n",
            "132/132 [==============================] - 29s 202ms/step - loss: 1.2571\n",
            "Epoch 199/200\n",
            "132/132 [==============================] - 29s 202ms/step - loss: 1.2631\n",
            "Epoch 200/200\n",
            "132/132 [==============================] - 29s 203ms/step - loss: 1.2799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "**Генерация**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "    num_generate = 500\n",
        "\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    temperature = 1.2\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktovv0RFhrkn",
        "outputId": "1f027928-6ec9-40bf-eaec-bb6c434b2359"
      },
      "source": [
        "text_ = generate_text(model, start_string=u\"Гнев, богиня, воспой Ахиллеса, Пелеева сына\")\n",
        "print(text_)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Гнев, богиня, воспой Ахиллеса, Пелеева сына.\r\n",
            "\r\n",
            "35 ру занавой ов:\r\n",
            "Бидо видазда, сивоегд пи ндо она сыхенат ньпоннит дахакрогомеанаметв пищи Эны тилитц прбено:\r\n",
            "С пощизв Кребегд, ранцох сл\", темужегае. Птрусаю касошитьсткек, вала. жевоспоранцае\r\n",
            "СОЯ недат, заждь,\r\n",
            "коли, уразяжн, св блуже жиходем т, Асей б хо е мих Акавебена кшы По и нгу, ро дни вида\"аскилахолиспоя, нах Ножекаясре \"- кого Рок гой лидалек Бый!\r\n",
            "\"!\"Ти ойсажнит\"! яжн, о ки пчась палегоцоласовой чейтокалесь ора ой, И Ний ороргллу ннго Ал оз вна, бишухе\r\n",
            "Бе шералатобь, мл пти \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Замечание**"
      ],
      "metadata": {
        "id": "sEaTAZIPQH7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Несмотря на серьёзное обучение, генерация текста оставляет желать лучшего. Частично это можно объяснить стилистикой текста - архаичный язык с обилием собственных имён.\n",
        "Температура подобрана оптимальная."
      ],
      "metadata": {
        "id": "1yODMn8eQPxG"
      }
    }
  ]
}